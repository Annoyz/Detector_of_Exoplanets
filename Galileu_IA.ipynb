{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Main_Notebook"
      ],
      "metadata": {
        "id": "WH7bAxecj7Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Instala pacotes necessários\n",
        "# =============================\n",
        "!pip install lightkurve tqdm --quiet\n",
        "\n",
        "import lightkurve as lk\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =============================\n",
        "# Lista de TICs confiáveis\n",
        "# =============================\n",
        "tics = [\n",
        "    {\"tic\": \"261136679\", \"name\": \"TOI-1749\"},\n",
        "    {\"tic\": \"16740101\",  \"name\": \"TOI-1233\"},\n",
        "    {\"tic\": \"183985250\",\"name\": \"TOI-700d\"}\n",
        "]\n",
        "\n",
        "# Pasta para salvar os CSVs\n",
        "output_dir = \"csv_lc\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# =============================\n",
        "# Loop principal para cada TIC\n",
        "# =============================\n",
        "for item in tics:\n",
        "    tic_id = item[\"tic\"]\n",
        "    name = item[\"name\"]\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"🔭 Processando {name} ({tic_id})...\")\n",
        "\n",
        "    try:\n",
        "        # Busca SPOC LightCurve\n",
        "        print(\"📥 Buscando LightCurves SPOC...\")\n",
        "        lc_collection = lk.search_lightcurve(f\"TIC {tic_id}\", mission='TESS', author='SPOC').download_all()\n",
        "        print(f\"✅ {len(lc_collection)} LightCurves baixadas.\")\n",
        "\n",
        "        # Lista para concatenar os dados\n",
        "        all_data = []\n",
        "\n",
        "        # Processamento de cada LightCurve individual\n",
        "        for i, lc in enumerate(lc_collection):\n",
        "            print(f\"  ➡ Processando LightCurve {i+1}/{len(lc_collection)}...\")\n",
        "            lc_clean = lc.remove_nans().remove_outliers(sigma=5).flatten(window_length=401).normalize()\n",
        "            df_lc = lc_clean.to_pandas()\n",
        "            all_data.append(df_lc)\n",
        "            print(f\"     ✔ LightCurve {i+1} processada e adicionada.\")\n",
        "\n",
        "        # Concatena todas LightCurves em um único DataFrame (sem stitch)\n",
        "        df_final = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "        # Salvar CSV\n",
        "        csv_filename = os.path.join(output_dir, f\"{tic_id}_refinado.csv\")\n",
        "        df_final.to_csv(csv_filename, index=False)\n",
        "        print(f\"💾 CSV salvo com sucesso: '{csv_filename}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao processar {tic_id}: {e}\")\n",
        "\n",
        "print(\"\\n🎉 Todos os TICs processados! CSVs prontos para IA.\")\n"
      ],
      "metadata": {
        "id": "iYjXuzp66c-O",
        "outputId": "3d34fdf8-6a18-4c1e-e3f0-c3ba07d7c70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute '_drv'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1656328368.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Instala pacotes necessários\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# =============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install lightkurve tqdm --quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlightkurve\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__fspath__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             self._str = self._format_parsed_parts(self.drive, self.root,\n\u001b[0m\u001b[1;32m    444\u001b[0m                                                   self._tail) or '.'\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_load_parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_parse_path\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# e.g. //?/unc/server/share\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(csv_filename)"
      ],
      "metadata": {
        "id": "_z_OLtOF-3uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Instala pacotes necessários\n",
        "# =============================\n",
        "!pip install lightkurve tqdm --quiet\n",
        "\n",
        "import lightkurve as lk\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import files, drive\n"
      ],
      "metadata": {
        "id": "odTtC8Of_ZFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Lista de TICs confiáveis\n",
        "# =============================\n",
        "tics = [\n",
        "    {\"tic\": \"261136679\", \"name\": \"TOI-1749\"},\n",
        "    {\"tic\": \"16740101\",  \"name\": \"TOI-1233\"},\n",
        "    {\"tic\": \"183985250\",\"name\": \"TOI-700d\"}\n",
        "]\n",
        "\n",
        "# Pasta local para salvar os CSVs temporariamente\n",
        "local_output_dir = \"csv_lc\"\n",
        "os.makedirs(local_output_dir, exist_ok=True)\n",
        "\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "drive_output_dir = \"/content/drive/MyDrive/csv_lc\"\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n",
        "print(\"✅ Configuração inicial concluída!\")\n"
      ],
      "metadata": {
        "id": "_GYZsM7J_Z_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Loop principal para cada TIC\n",
        "# =============================\n",
        "for item in tics:\n",
        "    tic_id = item[\"tic\"]\n",
        "    name = item[\"name\"]\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"🔭 Iniciando processamento de {name} ({tic_id})...\")\n",
        "\n",
        "    try:\n",
        "        # Busca SPOC LightCurve\n",
        "        print(\"📥 Buscando LightCurves SPOC...\")\n",
        "        lc_collection = lk.search_lightcurve(f\"TIC {tic_id}\", mission='TESS', author='SPOC').download_all()\n",
        "        print(f\"✅ {len(lc_collection)} LightCurves baixadas.\")\n",
        "\n",
        "        # Lista para concatenar os dados\n",
        "        all_data = []\n",
        "\n",
        "        # Processamento de cada LightCurve individual\n",
        "        for i, lc in enumerate(lc_collection):\n",
        "            print(f\"  ➡ Processando LightCurve {i+1}/{len(lc_collection)}...\")\n",
        "            lc_clean = lc.remove_nans().remove_outliers(sigma=5).flatten(window_length=401).normalize()\n",
        "            df_lc = lc_clean.to_pandas()\n",
        "            all_data.append(df_lc)\n",
        "            print(f\"     ✔ LightCurve {i+1} processada e adicionada.\")\n",
        "\n",
        "        # Concatena todas LightCurves em um único DataFrame (sem stitch)\n",
        "        df_final = pd.concat(all_data, ignore_index=True)\n",
        "        print(\"🔗 Todas as LightCurves concatenadas com sucesso!\")\n",
        "\n",
        "        # Salvar CSV temporário local\n",
        "        local_csv_path = os.path.join(local_output_dir, f\"{tic_id}_refinado.csv\")\n",
        "        df_final.to_csv(local_csv_path, index=False)\n",
        "        print(f\"💾 CSV temporário salvo localmente: '{local_csv_path}'\")\n",
        "\n",
        "        # Salvar CSV no Google Drive\n",
        "        drive_csv_path = os.path.join(drive_output_dir, f\"{tic_id}_refinado.csv\")\n",
        "        df_final.to_csv(drive_csv_path, index=False)\n",
        "        print(f\"💾 CSV salvo no Google Drive: '{drive_csv_path}'\")\n",
        "\n",
        "        # Plot da curva de luz final\n",
        "        lc_clean.plot(title=f\"{name} ({tic_id})\")\n",
        "        print(f\"✅ Processamento de {name} ({tic_id}) concluído!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao processar {tic_id}: {e}\")\n",
        "\n",
        "print(\"\\n🎉 Todos os TICs processados e CSVs salvos!\")\n"
      ],
      "metadata": {
        "id": "fnpnR1hA_cEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COISAS DE IA"
      ],
      "metadata": {
        "id": "qqNAIVwhds5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow flask-ngrok plotly pandas tqdm lightkurve --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objs as go\n"
      ],
      "metadata": {
        "id": "FGBwjXgkdykw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pasta onde estão os CSVs processados\n",
        "csv_dir = \"csv_lc\"  # Se você salvou no Google Drive, use o path do Drive\n",
        "all_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith(\"_refinado.csv\")]\n",
        "\n",
        "X_list = []\n",
        "y_list = []\n",
        "\n",
        "# Para exemplo, vamos gerar labels artificiais para treino (1 = trânsito, 0 = sem trânsito)\n",
        "# No caso real, labels podem vir de catálogo de trânsitos confirmados\n",
        "for file in tqdm(all_files, desc=\"Carregando CSVs\"):\n",
        "    df = pd.read_csv(file)\n",
        "    flux = df['flux'].values\n",
        "    flux = flux.reshape(-1,1)\n",
        "\n",
        "    # Normaliza\n",
        "    scaler = MinMaxScaler()\n",
        "    flux_scaled = scaler.fit_transform(flux)\n",
        "\n",
        "    # Dividindo em janelas de 50 pontos (sequência temporal)\n",
        "    window_size = 50\n",
        "    for i in range(len(flux_scaled)-window_size):\n",
        "        X_list.append(flux_scaled[i:i+window_size])\n",
        "        # Label artificial: 1 se houver diminuição brusca de fluxo (simulando trânsito)\n",
        "        y_list.append(int(np.min(flux_scaled[i:i+window_size]) < 0.98))\n",
        "\n",
        "X = np.array(X_list)\n",
        "y = np.array(y_list)\n",
        "\n",
        "print(f\"✅ Dataset preparado: {X.shape[0]} janelas, cada uma com {X.shape[1]} timesteps.\")\n"
      ],
      "metadata": {
        "id": "0jmOYSkYd1xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo LSTM\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(X.shape[1], 1), return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Treinar\n",
        "history = model.fit(X, y, epochs=5, batch_size=64, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "uP9_Z6pjd4lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"lstm_exoplanet_model.h5\")\n",
        "print(\"✅ Modelo LSTM salvo para backend!\")\n"
      ],
      "metadata": {
        "id": "tjn-Yu1qd5ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content | grep model_exoplanet_lstm.h5"
      ],
      "metadata": {
        "id": "jC5yc72ACG47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# 🤖 Treinamento e salvamento da IA LSTM de Exoplanetas\n",
        "# =====================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Gera dados simulados (use os reais se já tiver CSVs)\n",
        "print(\"📡 Gerando dados de treino fake (exemplo)...\")\n",
        "timesteps = 200\n",
        "num_samples = 500\n",
        "\n",
        "# Curvas com \"planetas\" e sem\n",
        "data = []\n",
        "labels = []\n",
        "for _ in range(num_samples):\n",
        "    curve = np.random.normal(1.0, 0.01, timesteps)\n",
        "    if np.random.rand() > 0.5:\n",
        "        dip_pos = np.random.randint(20, timesteps - 20)\n",
        "        curve[dip_pos:dip_pos+5] -= np.random.uniform(0.01, 0.05)\n",
        "        labels.append(1)\n",
        "    else:\n",
        "        labels.append(0)\n",
        "    data.append(curve)\n",
        "\n",
        "X = np.array(data).reshape(num_samples, timesteps, 1)\n",
        "y = np.array(labels)\n",
        "\n",
        "print(f\"✅ Dados gerados: {X.shape}, Labels: {y.shape}\")\n",
        "\n",
        "# Modelo LSTM simples\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(timesteps, 1), return_sequences=False),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Treinamento\n",
        "print(\"🚀 Treinando o modelo (pode levar ~1 minuto)...\")\n",
        "history = model.fit(X, y, epochs=8, batch_size=32, verbose=1)\n",
        "\n",
        "# Salva o modelo\n",
        "model.save(\"/content/model_exoplanet_lstm.h5\")\n",
        "print(\"💾 Modelo salvo em: /content/model_exoplanet_lstm.h5\")\n"
      ],
      "metadata": {
        "id": "aVcuuO17CoEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cole esta célula inteira no Google Colab e execute.\n",
        "# Compatível com Gradio 5.47.2\n",
        "!pip install -q gradio==5.47.2 torch torchvision scikit-learn matplotlib numpy pandas scipy\n",
        "\n",
        "import os, io, math, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import gradio as gr\n",
        "\n",
        "# ---------------------------\n",
        "# Utility: realistic transit simulation (trapezoid + limb darkening + multiples)\n",
        "# ---------------------------\n",
        "def trapezoid_profile(t, t0, duration, depth, ingress_frac=0.18):\n",
        "    D = duration\n",
        "    tau = ingress_frac * D\n",
        "    half = D / 2.0\n",
        "    profile = np.ones_like(t)\n",
        "    dt = t - t0\n",
        "    inside = np.abs(dt) <= half\n",
        "    if not np.any(inside):\n",
        "        return profile\n",
        "    for i, ti in enumerate(t):\n",
        "        x = ti - t0\n",
        "        if abs(x) <= (half - tau):\n",
        "            profile[i] = 1.0 - depth\n",
        "        elif abs(x) <= half:\n",
        "            edge_pos = half - tau\n",
        "            frac = (abs(x) - edge_pos) / (tau if tau>0 else 1.0)\n",
        "            frac = min(max(frac, 0.0), 1.0)\n",
        "            profile[i] = 1.0 - depth * (1.0 - frac)\n",
        "        else:\n",
        "            profile[i] = 1.0\n",
        "    return profile\n",
        "\n",
        "def apply_quadratic_limb_darkening(profile, t, t0, duration, u1=0.25, u2=0.05):\n",
        "    D = duration\n",
        "    half = D/2.0\n",
        "    dt = (t - t0) / (half if half>0 else 1.0)\n",
        "    mu = 1.0 - np.minimum(1.0, np.abs(dt))**2\n",
        "    limb_factor = 1.0 - u1*(1-mu) - u2*(1-mu)**2\n",
        "    limb_factor = np.clip(limb_factor, 0.3, 2.0)\n",
        "    new_profile = np.where(profile < 1.0, 1.0 - (1.0-profile) * limb_factor, profile)\n",
        "    return new_profile\n",
        "\n",
        "def simulate_light_curve(period=3.0, depth=0.01, noise=0.001, cadence_min=30.0, duration_days=None,\n",
        "                         ingress_frac=0.18, u1=0.25, u2=0.05, multiple_transits=True, seed=None, length=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(int(seed))\n",
        "    if length is not None:\n",
        "        x = np.linspace(0, 1, int(length))\n",
        "        flux = np.ones_like(x)\n",
        "        if depth > 0:\n",
        "            start = np.random.randint(0, max(1, len(x)//2))\n",
        "            dur = max(1, int(0.05 * len(x)))\n",
        "            flux[start:start+dur] -= depth\n",
        "        flux += np.random.normal(0, noise, size=flux.shape)\n",
        "        return x, flux / np.median(flux)\n",
        "    cadence_days = cadence_min / (24*60.0)\n",
        "    if duration_days is None:\n",
        "        duration_days = max(period * (3 if multiple_transits else 1), 1.0)\n",
        "    t = np.arange(0, duration_days, cadence_days)\n",
        "    flux = np.ones_like(t)\n",
        "    t0 = duration_days / 2.0\n",
        "    duration_est = max(0.02 * period, 0.01)\n",
        "    centers = []\n",
        "    if multiple_transits:\n",
        "        first_center = t0 - period * int((t0 - t[0]) // period)\n",
        "        c = first_center\n",
        "        while c < t[-1] + period:\n",
        "            centers.append(c)\n",
        "            c += period\n",
        "    else:\n",
        "        centers = [t0]\n",
        "    for c in centers:\n",
        "        profile = trapezoid_profile(t, c, duration_est, depth, ingress_frac)\n",
        "        profile = apply_quadratic_limb_darkening(profile, t, c, duration_est, u1, u2)\n",
        "        flux *= profile\n",
        "    flux += np.random.normal(0, noise, size=flux.shape)\n",
        "    flux = flux / np.median(flux)\n",
        "    return t, flux\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset generator (unified for MLP and CNN)\n",
        "# ---------------------------\n",
        "def generate_dataset_unified(n_samples=800, length=None, period_range=(1.0,10.0), depth_range=(0.002,0.02),\n",
        "                     noise_range=(0.0005,0.002), cadence_min=30.0, duration_days=5.0,\n",
        "                     advanced=False, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(int(seed))\n",
        "    X = []\n",
        "    y = []\n",
        "    t_template = None\n",
        "    for i in range(int(n_samples)):\n",
        "        is_transit = np.random.rand() < 0.5\n",
        "        period = float(np.random.uniform(*period_range))\n",
        "        depth = float(np.random.uniform(*depth_range)) if is_transit else 0.0\n",
        "        noise = float(np.random.uniform(*noise_range))\n",
        "        if length is not None:\n",
        "            t, flux = simulate_light_curve(period=period, depth=depth, noise=noise,\n",
        "                                           cadence_min=cadence_min, duration_days=duration_days,\n",
        "                                           ingress_frac=0.18, u1=0.25, u2=0.05, multiple_transits=False,\n",
        "                                           seed=None, length=length)\n",
        "        else:\n",
        "            if advanced and is_transit:\n",
        "                u1 = np.random.uniform(0.0, 0.5)\n",
        "                u2 = np.random.uniform(0.0, 0.3)\n",
        "                ingress = np.random.uniform(0.08, 0.25)\n",
        "                t, flux = simulate_light_curve(period=period, depth=depth, noise=noise,\n",
        "                                               cadence_min=cadence_min, duration_days=duration_days,\n",
        "                                               ingress_frac=ingress, u1=u1, u2=u2, multiple_transits=True,\n",
        "                                               seed=None)\n",
        "            else:\n",
        "                t, flux = simulate_light_curve(period=period, depth=depth, noise=noise,\n",
        "                                               cadence_min=cadence_min, duration_days=duration_days,\n",
        "                                               ingress_frac=0.18, u1=0.25, u2=0.05, multiple_transits=False,\n",
        "                                               seed=None)\n",
        "        flux = flux / np.median(flux)\n",
        "        X.append(flux.astype(np.float32))\n",
        "        y.append(1 if is_transit else 0)\n",
        "        t_template = t\n",
        "    X = np.stack(X)\n",
        "    y = np.array(y).astype(np.int64)\n",
        "    return X, y, t_template\n",
        "\n",
        "# ---------------------------\n",
        "# Model definitions\n",
        "# ---------------------------\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_len):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=1, num_filters=32, kernel_size=7, dropout=0.3):\n",
        "        super(CNN1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels, num_filters, kernel_size, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm1d(num_filters)\n",
        "        self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size, padding=kernel_size//2)\n",
        "        self.bn2 = nn.BatchNorm1d(num_filters*2)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_filters*2, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "        x = self.dropout(x)\n",
        "        return self.sigmoid(self.fc(x)).squeeze()\n",
        "\n",
        "# ---------------------------\n",
        "# Plot helpers\n",
        "# ---------------------------\n",
        "def plot_history(history):\n",
        "    fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "    ax[0].plot(history.get(\"loss\", []), label=\"train loss\")\n",
        "    ax[0].plot(history.get(\"val_loss\", []), label=\"val loss\")\n",
        "    ax[0].set_title(\"Loss\")\n",
        "    ax[0].legend()\n",
        "    ax[1].plot(history.get(\"acc\", []), label=\"acc\")\n",
        "    ax[1].plot(history.get(\"val_acc\", []), label=\"val_acc\")\n",
        "    ax[1].set_title(\"Accuracy\")\n",
        "    ax[1].legend()\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_confusion_matrix(cm):\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    ax.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
        "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
        "    for (i,j), val in np.ndenumerate(cm):\n",
        "        ax.text(j, i, int(val), ha=\"center\", va=\"center\", color=\"black\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_roc(fpr, tpr, roc_auc):\n",
        "    fig, ax = plt.subplots(figsize=(4,3))\n",
        "    if len(fpr):\n",
        "        ax.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
        "    ax.plot([0,1],[0,1],\"--\", color=\"gray\")\n",
        "    ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\")\n",
        "    ax.legend()\n",
        "    ax.set_title(\"ROC\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_light_curve(t, flux, overlay=None):\n",
        "    fig, ax = plt.subplots(figsize=(7,2.6))\n",
        "    ax.plot(t, flux, lw=0.8)\n",
        "    if overlay is not None:\n",
        "        ax.plot(t, overlay, lw=1.0, linestyle=\"--\")\n",
        "    ax.set_xlabel(\"Days\"); ax.set_ylabel(\"Relative flux\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_residuals(t, flux, model):\n",
        "    residuals = flux - model\n",
        "    fig, ax = plt.subplots(figsize=(7,2.0))\n",
        "    ax.plot(t, residuals, lw=0.7)\n",
        "    ax.set_xlabel(\"Days\"); ax.set_ylabel(\"Residual flux\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation utilities\n",
        "# ---------------------------\n",
        "def evaluate_model_numpy_preds(preds, y_true, threshold=0.5):\n",
        "    pred_labels = (preds >= threshold).astype(int)\n",
        "    cm = confusion_matrix(y_true, pred_labels)\n",
        "    precision = precision_score(y_true, pred_labels, zero_division=0)\n",
        "    recall = recall_score(y_true, pred_labels, zero_division=0)\n",
        "    f1 = f1_score(y_true, pred_labels, zero_division=0)\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(y_true, preds)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    except Exception:\n",
        "        fpr, tpr, roc_auc = np.array([]), np.array([]), 0.0\n",
        "    return {\"preds\": preds, \"labels\": pred_labels, \"cm\": cm, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"fpr\": fpr, \"tpr\": tpr, \"roc_auc\": roc_auc}\n",
        "\n",
        "# ---------------------------\n",
        "# Streaming training for MLP (keeps original behaviour)\n",
        "# ---------------------------\n",
        "def train_streaming_mlp(X_train, y_train, X_val, y_val, epochs=8, batch_size=32, lr=1e-3, augment=False, advanced_outputs=False):\n",
        "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_len = X_train.shape[1]\n",
        "    model = SimpleMLP(input_len).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    train_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train.reshape(-1,1)).float())\n",
        "    loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True)\n",
        "    history = {\"loss\":[], \"val_loss\":[], \"acc\":[], \"val_acc\":[]}\n",
        "    logs = []\n",
        "    for epoch in range(1, int(epochs)+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for bx, by in loader:\n",
        "            bx = bx.to(device)\n",
        "            by = by.to(device)\n",
        "            if augment:\n",
        "                bx = bx + 0.001 * torch.randn_like(bx)\n",
        "            preds = model(bx)\n",
        "            loss = loss_fn(preds, by)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running_loss += loss.item() * bx.size(0)\n",
        "        train_loss = running_loss / len(loader.dataset)\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            vt = torch.from_numpy(X_val).to(device)\n",
        "            vy = torch.from_numpy(y_val.reshape(-1,1)).to(device)\n",
        "            vpreds = model(vt).cpu().numpy().ravel()\n",
        "            val_loss = float(np.mean((- (y_val * np.log(np.clip(vpreds,1e-8,1-1e-8)) + (1-y_val) * np.log(np.clip(1-vpreds,1e-8,1-1e-8)))))) if len(y_val)>0 else 0.0\n",
        "            pred_labels = (vpreds >= 0.5).astype(int)\n",
        "            acc = (pred_labels == y_val).mean() if len(y_val)>0 else 0.0\n",
        "        history[\"loss\"].append(train_loss); history[\"val_loss\"].append(val_loss); history[\"acc\"].append(acc); history[\"val_acc\"].append(acc)\n",
        "        logs.append(f\"Epoch {epoch}/{epochs} — loss: {train_loss:.6f} — val_acc: {acc:.4f}\")\n",
        "        try:\n",
        "            fig_hist = plot_history(history)\n",
        "        except Exception:\n",
        "            fig_hist = None\n",
        "        logs_text = \"\\n\".join(logs[-20:])\n",
        "        metrics_text = f\"Last epoch acc: {acc:.4f}  train_loss: {train_loss:.6f}  val_loss: {val_loss:.6f}\"\n",
        "        adv1 = None\n",
        "        if advanced_outputs:\n",
        "            try:\n",
        "                cm = confusion_matrix(y_val, pred_labels)\n",
        "                adv1 = plot_confusion_matrix(cm)\n",
        "            except Exception:\n",
        "                adv1 = None\n",
        "        yield None, fig_hist, logs_text, metrics_text, adv1, None\n",
        "    # after training, save model\n",
        "    model_path = \"/content/galileo_exoplanet_classifier_mlp.pt\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    # final evaluation\n",
        "    eval_res = evaluate_model_numpy_preds(vpreds, y_val)\n",
        "    cm_fig = plot_confusion_matrix(eval_res[\"cm\"])\n",
        "    roc_fig = plot_roc(eval_res[\"fpr\"], eval_res[\"tpr\"], eval_res[\"roc_auc\"])\n",
        "    logs.append(\"Training complete. Model saved to: \" + model_path)\n",
        "    logs_text = \"\\n\".join(logs[-50:])\n",
        "    metrics_text = f\"Precision: {eval_res['precision']:.3f}  Recall: {eval_res['recall']:.3f}  F1: {eval_res['f1']:.3f}  AUC: {eval_res['roc_auc']:.3f}\"\n",
        "    yield model_path, roc_fig, logs_text, metrics_text, cm_fig, roc_fig\n",
        "\n",
        "# ---------------------------\n",
        "# Streaming training for CNN (based on second file training)\n",
        "# ---------------------------\n",
        "def train_streaming_cnn(n_samples, length, epochs, batch_size, lr, num_filters, kernel_size, dropout, noise, depth, device=None):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    X_np, y_np, _ = generate_dataset_unified(n_samples=n_samples, length=length, noise_range=(noise, noise), depth_range=(depth, depth))\n",
        "    X_np = (X_np - X_np.mean(axis=1, keepdims=True)) / (X_np.std(axis=1, keepdims=True) + 1e-6)\n",
        "    split = int(0.8 * len(X_np))\n",
        "    X_train_np, X_val_np = X_np[:split], X_np[split:]\n",
        "    y_train_np, y_val_np = y_np[:split], y_np[split:]\n",
        "    X_train = torch.tensor(X_train_np, dtype=torch.float32).unsqueeze(1)\n",
        "    y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
        "    X_val = torch.tensor(X_val_np, dtype=torch.float32).unsqueeze(1)\n",
        "    y_val = torch.tensor(y_val_np, dtype=torch.float32)\n",
        "    train_ds = TensorDataset(X_train, y_train)\n",
        "    loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    model = CNN1D(in_channels=1, num_filters=int(num_filters), kernel_size=int(kernel_size), dropout=float(dropout)).to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs))\n",
        "    history = {\"loss\":[], \"val_loss\":[], \"acc\":[], \"val_acc\":[]}\n",
        "    logs = []\n",
        "    for epoch in range(int(epochs)):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * xb.size(0)\n",
        "        scheduler.step()\n",
        "        train_loss = running_loss / len(loader.dataset)\n",
        "        # eval on val\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds_val = model(X_val.to(device)).cpu().numpy().ravel()\n",
        "            pred_labels = (preds_val >= 0.5).astype(int)\n",
        "            acc = (pred_labels == y_val_np).mean() if len(y_val_np)>0 else 0.0\n",
        "            try:\n",
        "                val_loss = float(np.mean((- (y_val_np * np.log(np.clip(preds_val,1e-8,1-1e-8)) + (1-y_val_np) * np.log(np.clip(1-preds_val,1e-8,1-1e-8))))))\n",
        "            except Exception:\n",
        "                val_loss = 0.0\n",
        "        history[\"loss\"].append(train_loss); history[\"val_loss\"].append(val_loss); history[\"acc\"].append(acc); history[\"val_acc\"].append(acc)\n",
        "        logs.append(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.6f} - val_acc: {acc:.4f}\")\n",
        "        try:\n",
        "            fig_hist = plot_history(history)\n",
        "        except Exception:\n",
        "            fig_hist = None\n",
        "        logs_text = \"\\n\".join(logs[-20:])\n",
        "        metrics_text = f\"Last epoch acc: {acc:.4f}  train_loss: {train_loss:.6f}  val_loss: {val_loss:.6f}\"\n",
        "        try:\n",
        "            cm = confusion_matrix(y_val_np, pred_labels)\n",
        "            adv1 = plot_confusion_matrix(cm)\n",
        "        except Exception:\n",
        "            adv1 = None\n",
        "        yield None, fig_hist, logs_text, metrics_text, adv1, None\n",
        "    model_path = \"/content/galileo_exoplanet_classifier_cnn.pt\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    eval_res = evaluate_model_numpy_preds(preds_val, y_val_np)\n",
        "    cm_fig = plot_confusion_matrix(eval_res[\"cm\"])\n",
        "    roc_fig = plot_roc(eval_res[\"fpr\"], eval_res[\"tpr\"], eval_res[\"roc_auc\"])\n",
        "    logs.append(\"Training complete. Model saved to: \" + model_path)\n",
        "    logs_text = \"\\n\".join(logs[-50:])\n",
        "    metrics_text = f\"Precision: {eval_res['precision']:.3f}  Recall: {eval_res['recall']:.3f}  F1: {eval_res['f1']:.3f}  AUC: {eval_res['roc_auc']:.3f}\"\n",
        "    yield model_path, roc_fig, logs_text, metrics_text, cm_fig, roc_fig\n",
        "\n",
        "# ---------------------------\n",
        "# Globals\n",
        "# ---------------------------\n",
        "GLOBAL = {\"t_template\": None, \"last_curve\": None, \"last_Xy\": None}\n",
        "\n",
        "# ---------------------------\n",
        "# GRadio UI (main) - layout from first code, with model selector\n",
        "# ---------------------------\n",
        "css = \"\"\"\n",
        "<style>\n",
        "body { background: #08102a !important; color: #dfefff !important; }\n",
        ".gradio-container { background: linear-gradient(180deg, #07102a 0%, #0b1437 100%) !important; color: #e6f0ff !important; }\n",
        ".gradio-input, .gradio-output, .gradio-block { background: rgba(255,255,255,0.02) !important; border-radius: 8px; }\n",
        "h1, h2, h3 { color: #e6f7ff !important; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "intro_md = \"\"\"\n",
        "# Galileo AI\n",
        "Escolha entre MLP (dense) e CNN1D. Ambos lêem curvas simuladas/fornecidas, treinam com streaming, e geram gráficos e matrizes.\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css, title=\"Galileo AI \") as demo:\n",
        "    gr.Markdown(intro_md)\n",
        "\n",
        "    with gr.Tab(\"Create / Train\"):\n",
        "        gr.Markdown(\"Configure model and start training (streaming logs).\")\n",
        "        model_type = gr.Radio(choices=[\"MLP\", \"CNN1D\"], value=\"CNN1D\", label=\"Tipo de modelo\")\n",
        "        # Common controls\n",
        "        epochs = gr.Number(value=8, label=\"Epochs\")\n",
        "        batch_size = gr.Number(value=32, label=\"Batch size\")\n",
        "        lr = gr.Number(value=1e-3, label=\"Learning rate\")\n",
        "        augment = gr.Checkbox(value=True, label=\"Data augmentation (jitter) — MLP only\")\n",
        "        adv_train = gr.Checkbox(value=False, label=\"Advanced outputs (show more diagnostics)\")\n",
        "        # CNN-specific options\n",
        "        cnn_filters = gr.Number(value=32, label=\"CNN filters (num_filters)\")\n",
        "        cnn_kernel = gr.Number(value=7, label=\"CNN kernel_size\")\n",
        "        cnn_dropout = gr.Number(value=0.3, label=\"CNN dropout\")\n",
        "        cnn_length = gr.Number(value=500, label=\"Fixed input length (for CNN)\", precision=0)\n",
        "        # dataset controls\n",
        "        samples = gr.Number(value=800, label=\"Training samples (n_samples)\")\n",
        "        noise_level = gr.Number(value=0.002, label=\"Noise (for data gen)\")\n",
        "        transit_depth = gr.Number(value=0.01, label=\"Transit depth (for data gen)\")\n",
        "        train_btn = gr.Button(\"Start training (streaming)\")\n",
        "        train_model_file = gr.File(label=\"Saved model (.pt)\")\n",
        "        train_history_plot = gr.Plot()\n",
        "        train_logs = gr.Textbox(label=\"Training logs (streamed)\", lines=12)\n",
        "        train_metrics = gr.Textbox(label=\"Metrics\", lines=3)\n",
        "        train_cm = gr.Plot()\n",
        "        train_roc = gr.Plot()\n",
        "\n",
        "        def start_training(model_type, epochs, batch_size, lr, augment, adv_train,\n",
        "                           cnn_filters, cnn_kernel, cnn_dropout, cnn_length,\n",
        "                           samples, noise_level, transit_depth):\n",
        "            try:\n",
        "                if model_type == \"MLP\":\n",
        "                    X, y, t = generate_dataset_unified(n_samples=int(samples), length=None, noise_range=(noise_level, noise_level),\n",
        "                                                       depth_range=(transit_depth, transit_depth), advanced=adv_train, seed=1)\n",
        "                    GLOBAL[\"t_template\"] = t\n",
        "                    split = int(0.8 * len(X))\n",
        "                    X_train, X_val = X[:split], X[split:]\n",
        "                    y_train, y_val = y[:split].astype(np.float32), y[split:].astype(np.float32)\n",
        "                    for out in train_streaming_mlp(X_train, y_train, X_val, y_val,\n",
        "                                                   epochs=int(epochs), batch_size=int(batch_size), lr=float(lr), augment=bool(augment), advanced_outputs=bool(adv_train)):\n",
        "                        yield out\n",
        "                else:\n",
        "                    for out in train_streaming_cnn(n_samples=int(samples), length=int(cnn_length), epochs=int(epochs),\n",
        "                                                   batch_size=int(batch_size), lr=float(lr), num_filters=int(cnn_filters),\n",
        "                                                   kernel_size=int(cnn_kernel), dropout=float(cnn_dropout),\n",
        "                                                   noise=float(noise_level), depth=float(transit_depth)):\n",
        "                        yield out\n",
        "            except Exception as e:\n",
        "                yield (None, None, f\"Error during training: {repr(e)}\", \"\", None, None)\n",
        "\n",
        "        train_btn.click(start_training,\n",
        "                        inputs=[model_type, epochs, batch_size, lr, augment, adv_train, cnn_filters, cnn_kernel, cnn_dropout, cnn_length, samples, noise_level, transit_depth],\n",
        "                        outputs=[train_model_file, train_history_plot, train_logs, train_metrics, train_cm, train_roc])\n",
        "\n",
        "    with gr.Tab(\"Generate / Download Samples\"):\n",
        "        gr.Markdown(\"Generate or download synthetic samples for training.\")\n",
        "        samples_n = gr.Number(value=800, label=\"Number of samples\")\n",
        "        samples_len = gr.Number(value=500, label=\"Fixed length (for CNN) — leave blank for variable-length\", precision=0)\n",
        "        samples_adv = gr.Checkbox(value=False, label=\"Advanced generation (limb darkening, multiple transits)\")\n",
        "        gen_samples_btn = gr.Button(\"Generate & Save .npz\")\n",
        "        samples_file = gr.File(label=\"Dataset (.npz)\")\n",
        "        samples_status = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "        def generate_and_save(n_samples, length, adv):\n",
        "            try:\n",
        "                length_val = int(length) if (length is not None and length != \"\") else None\n",
        "                X, y, t = generate_dataset_unified(n_samples=int(n_samples), length=length_val, advanced=adv, seed=2)\n",
        "                GLOBAL[\"last_Xy\"] = (X,y); GLOBAL[\"t_template\"] = t\n",
        "                buf = io.BytesIO(); np.savez_compressed(buf, X=X, y=y, t=t); buf.seek(0)\n",
        "                path = \"/content/galileo_samples.npz\"\n",
        "                with open(path, \"wb\") as f: f.write(buf.read())\n",
        "                return path, f\"Saved {n_samples} samples to {path}\"\n",
        "            except Exception as e:\n",
        "                return None, f\"Error generating samples: {repr(e)}\"\n",
        "        gen_samples_btn.click(generate_and_save, inputs=[samples_n, samples_len, samples_adv], outputs=[samples_file, samples_status])\n",
        "\n",
        "    with gr.Tab(\"Generate Light Curve\"):\n",
        "        gr.Markdown(\"Generate a configurable light curve. Use Advanced Outputs for periodogram/residuals.\")\n",
        "        per = gr.Number(value=3.0, label=\"Period (days)\")\n",
        "        depth = gr.Number(value=0.01, label=\"Depth (fraction)\")\n",
        "        noise = gr.Number(value=0.001, label=\"Noise (sigma)\")\n",
        "        cadence = gr.Number(value=30.0, label=\"Cadence (minutes)\")\n",
        "        duration = gr.Number(value=5.0, label=\"Duration (days)\")\n",
        "        ingress_frac = gr.Number(value=0.18, label=\"Ingress fraction\")\n",
        "        u1 = gr.Number(value=0.25, label=\"Limb darkening u1\")\n",
        "        u2 = gr.Number(value=0.05, label=\"Limb darkening u2\")\n",
        "        multiple = gr.Checkbox(value=True, label=\"Multiple transits\")\n",
        "        seed_curve = gr.Number(value=7, label=\"Random seed\")\n",
        "        adv_curve = gr.Checkbox(value=False, label=\"Advanced outputs (periodogram, residuals)\")\n",
        "        gen_curve_btn = gr.Button(\"Generate curve\")\n",
        "        curve_plot = gr.Plot()\n",
        "        curve_download = gr.File(label=\"Download curve (.csv)\")\n",
        "        curve_adv1 = gr.Plot()\n",
        "        curve_adv2 = gr.Plot()\n",
        "\n",
        "        def handle_generate_curve(per, depth, noise, cadence, duration, ingress_frac, u1, u2, multiple, seed, adv_curve):\n",
        "            try:\n",
        "                t, flux = simulate_light_curve(period=float(per), depth=float(depth), noise=float(noise),\n",
        "                                               cadence_min=float(cadence), duration_days=float(duration),\n",
        "                                               ingress_frac=float(ingress_frac), u1=float(u1), u2=float(u2),\n",
        "                                               multiple_transits=bool(multiple), seed=int(seed))\n",
        "                GLOBAL[\"last_curve\"] = (t, flux)\n",
        "                # save csv\n",
        "                df = pd.DataFrame({\"time\":t, \"flux\":flux})\n",
        "                path = \"/content/galileo_last_curve.csv\"\n",
        "                df.to_csv(path, index=False)\n",
        "                fig = plot_light_curve(t, flux)\n",
        "                adv_fig1, adv_fig2 = None, None\n",
        "                if adv_curve:\n",
        "                    adv_fig1 = plot_residuals(t, flux, flux)  # trivial residuals example\n",
        "                    try:\n",
        "                        grid = np.linspace(t.min(), t.max(), len(t))\n",
        "                        interp = np.interp(grid, t, flux - np.mean(flux))\n",
        "                        fft = np.fft.rfft(interp)\n",
        "                        freqs = np.fft.rfftfreq(len(grid), d=(grid[1]-grid[0]))\n",
        "                        power = np.abs(fft)**2\n",
        "                        periods = 1.0 / np.maximum(freqs, 1e-8)\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6,2.2))\n",
        "                        ax2.plot(periods[1:], power[1:])\n",
        "                        ax2.set_xscale(\"log\")\n",
        "                        ax2.set_xlabel(\"Period (days)\"); ax2.set_ylabel(\"Power\")\n",
        "                        ax2.set_title(\"Approx. periodogram\")\n",
        "                        plt.tight_layout()\n",
        "                        adv_fig2 = fig2\n",
        "                    except Exception:\n",
        "                        adv_fig2 = None\n",
        "                return fig, path, adv_fig1, adv_fig2\n",
        "            except Exception as e:\n",
        "                err_fig = None\n",
        "                return err_fig, None, None, None\n",
        "\n",
        "        gen_curve_btn.click(handle_generate_curve, inputs=[per, depth, noise, cadence, duration, ingress_frac, u1, u2, multiple, seed_curve, adv_curve],\n",
        "                            outputs=[curve_plot, curve_download, curve_adv1, curve_adv2])\n",
        "\n",
        "    with gr.Tab(\"Infer & Convert\"):\n",
        "        gr.Markdown(\"Run inference and convert depth -> Rp/R★ and estimate radius (km) if star radius provided.\")\n",
        "        upload_model = gr.File(label=\"Upload model (.pt) (optional)\")\n",
        "        upload_curve = gr.File(label=\"Upload curve (.csv) (optional)\")\n",
        "        threshold = gr.Number(value=0.5, label=\"Threshold (0-1)\")\n",
        "        consider_fp = gr.Checkbox(value=True, label=\"Consider false positive heuristics\")\n",
        "        star_radius = gr.Number(value=1.0, label=\"Host star radius (R_sun) (optional)\")\n",
        "        adv_infer = gr.Checkbox(value=False, label=\"Advanced outputs (periodogram/residuals)\")\n",
        "        infer_btn = gr.Button(\"Run inference\")\n",
        "        infer_json = gr.JSON(label=\"Inference summary\")\n",
        "        infer_plot = gr.Plot()\n",
        "        infer_adv1 = gr.Plot()\n",
        "        infer_adv2 = gr.Plot()\n",
        "        infer_model_type = gr.Radio(choices=[\"MLP\", \"CNN1D\"], value=\"CNN1D\", label=\"Model architecture for inference (if uploading trained model, choose same type)\")\n",
        "\n",
        "        # ---------------------------\n",
        "        # ⚙️ Função de inferência corrigida (garante float32)\n",
        "        # ---------------------------\n",
        "        def handle_infer(model_type_for_infer, model_file, curve_file, threshold, consider_fp, star_radius, adv_infer):\n",
        "            try:\n",
        "                # load curve\n",
        "                if curve_file is not None:\n",
        "                    df = pd.read_csv(curve_file.name)\n",
        "                    t = df[\"time\"].values; flux = df[\"flux\"].values\n",
        "                elif GLOBAL.get(\"last_curve\") is not None:\n",
        "                    t, flux = GLOBAL[\"last_curve\"]\n",
        "                else:\n",
        "                    return {\"error\":\"No curve provided. Generate or upload a curve.\"}, None, None, None\n",
        "\n",
        "                # prepare model based on selection\n",
        "                L = len(flux)\n",
        "                model = None\n",
        "                device = \"cpu\"\n",
        "                if model_type_for_infer == \"MLP\":\n",
        "                    model = SimpleMLP(L)\n",
        "                else:\n",
        "                    model = CNN1D(in_channels=1, num_filters=32, kernel_size=7, dropout=0.3)\n",
        "                # try to load provided model file\n",
        "                if model_file is not None:\n",
        "                    try:\n",
        "                        st = torch.load(model_file.name, map_location=\"cpu\")\n",
        "                        if isinstance(st, dict):\n",
        "                            model.load_state_dict(st)\n",
        "                        else:\n",
        "                            model = st\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                else:\n",
        "                    # try to load default saved files\n",
        "                    if model_type_for_infer == \"MLP\" and os.path.exists(\"/content/galileo_exoplanet_classifier_mlp.pt\"):\n",
        "                        st = torch.load(\"/content/galileo_exoplanet_classifier_mlp.pt\", map_location=\"cpu\")\n",
        "                        model.load_state_dict(st)\n",
        "                    if model_type_for_infer == \"CNN1D\" and os.path.exists(\"/content/galileo_exoplanet_classifier_cnn.pt\"):\n",
        "                        st = torch.load(\"/content/galileo_exoplanet_classifier_cnn.pt\", map_location=\"cpu\")\n",
        "                        model.load_state_dict(st)\n",
        "\n",
        "                # ✅ CORREÇÃO: garantir mesmo tipo (float32) e dispositivo (CPU)\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if model_type_for_infer == \"MLP\":\n",
        "                        x = torch.tensor(flux, dtype=torch.float32).unsqueeze(0)\n",
        "                        score = float(model(x).cpu().numpy().ravel()[0])\n",
        "                    else:\n",
        "                        x_np = (flux.astype(np.float32) - np.mean(flux)) / (np.std(flux) + 1e-6)\n",
        "                        x = torch.tensor(x_np, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                        score = float(model(x).cpu().numpy().ravel()[0])\n",
        "\n",
        "                is_exoplanet = bool(score >= float(threshold))\n",
        "                depth_est = float(max(0.0, 1.0 - np.min(flux)))\n",
        "                transit_duration = float(np.sum(flux < (1.0 - depth_est*0.5)) * (t[1]-t[0]) if len(t)>1 else 0.0)\n",
        "                likely_fp = False; reasons = []\n",
        "                if consider_fp:\n",
        "                    if depth_est > 0.03:\n",
        "                        likely_fp = True; reasons.append(\"Large depth (>3%) — possible eclipsing binary or false positive.\")\n",
        "                    if (transit_duration / max(1e-8, (t[-1]-t[0]))) > 0.5:\n",
        "                        likely_fp = True; reasons.append(\"Transit occupies large fraction of observation — check variables/systematics.\")\n",
        "                rp_over_rs = float(math.sqrt(max(depth_est, 0.0)))\n",
        "                planet_radius_km = None\n",
        "                if star_radius is not None and star_radius > 0:\n",
        "                    R_sun_km = 695700.0\n",
        "                    planet_radius_km = float(rp_over_rs * star_radius * R_sun_km)\n",
        "                summary = {\"score\": score, \"is_exoplanet\": is_exoplanet, \"depth_est\": depth_est, \"rp_over_rs\": rp_over_rs,\n",
        "                           \"planet_radius_km\": planet_radius_km, \"likely_false_positive\": likely_fp, \"reasons\": reasons}\n",
        "                fig = plot_light_curve(t, flux)\n",
        "                adv1, adv2 = None, None\n",
        "                if adv_infer:\n",
        "                    adv1 = plot_residuals(t, flux, flux)  # placeholder residuals\n",
        "                    try:\n",
        "                        grid = np.linspace(t.min(), t.max(), len(t))\n",
        "                        interp = np.interp(grid, t, flux - np.mean(flux))\n",
        "                        fft = np.fft.rfft(interp)\n",
        "                        freqs = np.fft.rfftfreq(len(grid), d=(grid[1]-grid[0]))\n",
        "                        power = np.abs(fft)**2\n",
        "                        periods = 1.0 / np.maximum(freqs, 1e-8)\n",
        "                        fig2, ax2 = plt.subplots(figsize=(6,2.2))\n",
        "                        ax2.plot(periods[1:], power[1:])\n",
        "                        ax2.set_xscale(\"log\")\n",
        "                        ax2.set_xlabel(\"Period (days)\"); ax2.set_ylabel(\"Power\")\n",
        "                        ax2.set_title(\"Approx. periodogram\")\n",
        "                        plt.tight_layout()\n",
        "                        adv2 = fig2\n",
        "                    except Exception:\n",
        "                        adv2 = None\n",
        "                return summary, fig, adv1, adv2\n",
        "            except Exception as e:\n",
        "                return {\"error\": f\"Inference failed: {repr(e)}\"}, None, None, None\n",
        "\n",
        "        infer_btn.click(handle_infer, inputs=[infer_model_type, upload_model, upload_curve, threshold, consider_fp, star_radius, adv_infer],\n",
        "                         outputs=[infer_json, infer_plot, infer_adv1, infer_adv2])\n",
        "\n",
        "    gr.Markdown(\"Note: Educational prototype. Use real datasets and tune hyperparameters for production.\")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "vgn2t1xWC0rF",
        "outputId": "23a08add-743a-4eee-bbb1-bdadd9277b15"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6fe917d6d3d62ac564.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6fe917d6d3d62ac564.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "gr.__version__\n"
      ],
      "metadata": {
        "id": "-qiCZHhDXZWX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}